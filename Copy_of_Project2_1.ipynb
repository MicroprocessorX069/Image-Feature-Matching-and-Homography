{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Project2.1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MicroprocessorX069/Image-Feature-Matching-and-Homography/blob/master/Copy_of_Project2_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "iZYTjw3S2fYp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install opencv-python==3.4.2.16\n",
        "!pip install opencv-contrib-python==3.4.2.16"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "htSNVWvyOY1u",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2 as cv\n",
        "from matplotlib import pyplot as plt\n",
        "import random\n",
        "\n",
        "def findKeypoints(input_name,output_name):\n",
        "  img = cv.imread(input_name)\n",
        "  gray= cv.cvtColor(img,cv.COLOR_BGR2GRAY)\n",
        "  sift = cv.xfeatures2d.SIFT_create()\n",
        "  kp, des = sift.detectAndCompute(gray,None)\n",
        "  img=cv.drawKeypoints(gray,kp,img)\n",
        "  #cv.imshow(\"Image\", img)\n",
        "  #cv.waitKey(0)\n",
        "  #cv.destroyAllWindows()\n",
        "  cv.imwrite(output_name,img)\n",
        "\n",
        "\n",
        "def knnMatching(input_name1,input_name2,output_name):\n",
        "  img1 = cv.imread(input_name1,0)          # queryImage\n",
        "  img2 = cv.imread(input_name2,0) # trainImage\n",
        "  # Initiate SIFT detector\n",
        "  sift = cv.xfeatures2d.SIFT_create()\n",
        "  # find the keypoints and descriptors with SIFT\n",
        "  kp1, des1 = sift.detectAndCompute(img1,None)\n",
        "  kp2, des2 = sift.detectAndCompute(img2,None)\n",
        "  # BFMatcher with default params\n",
        "  bf = cv.BFMatcher()\n",
        "  matches = bf.knnMatch(des1,des2, k=2)\n",
        "  # Apply ratio test\n",
        "  good = []\n",
        "  for m,n in matches:\n",
        "      if m.distance < 0.75*n.distance:\n",
        "          good.append([m])\n",
        "  img3=np.array([])       \n",
        "  # cv.drawMatchesKnn expects list of lists as matches.\n",
        "  img3 = cv.drawMatchesKnn(img1,kp1,img2,kp2,good,outImg=img3,flags=2)\n",
        "  cv.imwrite(output_name,img3)\n",
        " \n",
        "def calculateHomography(input_name1,input_name2):\n",
        "  Min_matches=10\n",
        "  img1=cv.imread(input_name1,0)\n",
        "  img2=cv.imread(input_name2,0)\n",
        "\n",
        "  sift=cv.xfeatures2d.SIFT_create()\n",
        "  kp1, des1=sift.detectAndCompute(img1, None) # Here mask is None.\n",
        "  kp2, des2=sift.detectAndCompute(img2, None)\n",
        "\n",
        "  Flann_index=1\n",
        "  #dict() is just a function to create a dictionary of arguments \n",
        "  #More arguments can be kept than required by the method\n",
        "\n",
        "  index_param= dict(algorithm=Flann_index,trees=5)\n",
        "  search_param=dict(checks=50)\n",
        "  flann=cv.FlannBasedMatcher(index_param,search_param)\n",
        "  matches=flann.knnMatch(des1,des2,k=2)\n",
        "  good_matches=[]\n",
        "  for m,n in matches:\n",
        "    if m.distance<0.7*n.distance:\n",
        "      good_matches.append(m)\n",
        "\n",
        "  #Set a condition of n matches to find an object\n",
        "  #If found, extract the locations, to find the perspective transform\n",
        "\n",
        "  if(len(good_matches)>Min_matches):\n",
        "    #queryIdx and trainIdx are the descriptors of the good matched locations\n",
        "    src_pts=np.float32([kp1[m.queryIdx].pt for m in good_matches]).reshape(-1,1,2)\n",
        "    dst_pts=np.float32([kp2[m.trainIdx].pt for m in good_matches]).reshape(-1,1,2)\n",
        "\n",
        "    M,mask=cv.findHomography(src_pts,dst_pts,cv.RANSAC,5.0)\n",
        "    return M, mask\n",
        "  \n",
        "def featureMatching(input_name1,input_name2,output_name):\n",
        "  Min_matches=10\n",
        "  img1=cv.imread(input_name1,0)\n",
        "  img2=cv.imread(input_name2,0)\n",
        "\n",
        "  sift=cv.xfeatures2d.SIFT_create()\n",
        "  kp1, des1=sift.detectAndCompute(img1, None) # Here mask is None.\n",
        "  kp2, des2=sift.detectAndCompute(img2, None)\n",
        "\n",
        "  Flann_index=1\n",
        "  #dict() is just a function to create a dictionary of arguments \n",
        "  #More arguments can be kept than required by the method\n",
        "\n",
        "  index_param= dict(algorithm=Flann_index,trees=5)\n",
        "  search_param=dict(checks=50)\n",
        "  flann=cv.FlannBasedMatcher(index_param,search_param)\n",
        "  matches=flann.knnMatch(des1,des2,k=2)\n",
        "  good_matches=[]\n",
        "  for m,n in matches:\n",
        "    if m.distance<0.7*n.distance:\n",
        "      good_matches.append(m)\n",
        "\n",
        "  #Set a condition of n matches to find an object\n",
        "  #If found, extract the locations, to find the perspective transform\n",
        "\n",
        "  if(len(good_matches)>Min_matches):\n",
        "    #queryIdx and trainIdx are the descriptors of the good matched locations\n",
        "    src_pts=np.float32([kp1[m.queryIdx].pt for m in good_matches]).reshape(-1,1,2)\n",
        "    dst_pts=np.float32([kp2[m.trainIdx].pt for m in good_matches]).reshape(-1,1,2)\n",
        "\n",
        "    M,mask=cv.findHomography(src_pts,dst_pts,cv.RANSAC,5.0)\n",
        "    matchesMask=mask.ravel().tolist()\n",
        "    inliers=np.where(np.array(matchesMask)==1)[0]\n",
        "    size_inliers=int(inliers.shape[0])\n",
        "    matchesMask=np.zeros(np.array(matchesMask).shape)\n",
        "    for i in range(0,10):\n",
        "      random1=random.randint(0,size_inliers)\n",
        "      matchesMask[inliers[random1]]=1\n",
        "    matchesMask=matchesMask.ravel().tolist()\n",
        "    h,w=img1.shape\n",
        "    pts=np.float32([[0,0],[0,h-1],[w-1,h-1],[w-1,0]]).reshape(-1,1,2)\n",
        "    dst=cv.perspectiveTransform(pts,M)\n",
        "\n",
        "    img2=cv.polylines(img2,[np.int32(dst)],True,255,3,cv.LINE_AA)\n",
        "\n",
        "  else:\n",
        "    print(\"not enoug matches\")\n",
        "    matchesMask=None\n",
        "\n",
        "  draw_param=dict(matchColor=(0,255,0), singlePointColor=None, matchesMask= matchesMask, flags=2)\n",
        "  imgQ4= cv.drawMatches(img1,kp1,img2,kp2,good_matches,None,**draw_param)\n",
        "  cv.imwrite(output_name,imgQ4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tpfrL3VLorhB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Keypoint detection\n",
        "Keypoints between two images are matched by identifying their nearest neighbours. But in some cases, the second closest-match may be very near to the first.\n",
        "\n",
        "**sift.detect()** function finds the keypoint in the images. You can pass a mask if you want to search only a part of image. Each keypoint is a special structure which has many attributes like its (x,y) coordinates, size of the meaningful neighbourhood, angle which specifies its orientation, response that specifies strength of keypoints etc.\n",
        "\n",
        "OpenCV also provides **cv.drawKeyPoints()** function which draws the small circles on the locations of keypoints.\n",
        "\n",
        "Now to calculate the descriptor, OpenCV provides two methods.\n",
        "\n",
        "1. Since you already found keypoints, you can call sift.compute() which computes the descriptors from the keypoints we have found. Eg: kp,des = sift.compute(gray,kp)\n",
        "2. If you didn't find keypoints, directly find keypoints and descriptors in a single step with the function, sift.detectAndCompute().\n"
      ]
    },
    {
      "metadata": {
        "id": "6zZP2fCbOd3m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#1.1\n",
        "findKeypoints(\"mountain1.jpg\",\"task1_sift1.jpg\")\n",
        "findKeypoints(\"mountain2.jpg\",\"task1_sift2.jpg\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GexNrrx5pRlj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Feature Matching using KNN\n",
        "\n",
        "Brute-Force takes the descriptor of one feature in first set and is matched with all other features in second set using some distance calculation. And the closest one is returned.\n",
        "\n",
        "For BF matcher, first we have to create the BFMatcher object using cv2.BFMatcher().\n",
        "\n",
        "Second method returns k best matches where k is specified by the user. It may be useful when we need to do additional work on that.\n",
        "\n",
        "Like we used cv2.drawKeypoints() to draw keypoints, cv2.drawMatches() helps us to draw the matches. It stacks two images horizontally and draw lines from first image to second image showing best matches. There is also cv2.drawMatchesKnn which draws all the k best matches. If k=2, it will draw two match-lines for each keypoint."
      ]
    },
    {
      "metadata": {
        "id": "s73Nv-ROolFp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#1.2blob:https://colab.research.google.com/411a2ada-7cba-4ce3-bbd3-0c45350230c6\n",
        "knnMatching(\"mountain1.jpg\",\"mountain2.jpg\",\"task1_matches_knn.jpg\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FqzgP3ynqKsU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Calculating homography matrix and masks\n",
        "\n",
        "Homography matrix is a 3x3 matrix but with 8 DoF (degrees of freedom) as it is estimated up to a scale. It is generally normalized (see also 1) with h33=1 or h211+h212+h213+h221+h222+h223+h231+h232+h233=1.\n",
        "\n",
        "FLANN stands for Fast Library for Approximate Nearest Neighbors. It contains a collection of algorithms optimized for fast nearest neighbor search in large datasets and for high dimensional features."
      ]
    },
    {
      "metadata": {
        "id": "e5j2OoiaopFQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#1.3\n",
        "M, mask= calculateHomography(\"mountain1.jpg\",\"mountain2.jpg\")\n",
        "M"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BN8Pna-erET1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Feature Matching \n",
        "\n",
        "FLANN stands for Fast Library for Approximate Nearest Neighbors. It contains a collection of algorithms optimized for fast nearest neighbor search in large datasets and for high dimensional features. It works more faster than BFMatcher for large datasets. We will see the second example with FLANN based matcher.\n",
        "\n",
        "For FLANN based matcher, we need to pass two dictionaries which specifies the algorithm to be used, its related parameters etc"
      ]
    },
    {
      "metadata": {
        "id": "e1dVxd4Gol2I",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#1.4\n",
        "featureMatching(\"mountain1.jpg\",\"mountain2.jpg\",\"task1_matches.jpg\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OFefvhNMKImd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Q1.5.\n",
        "imgQ42 = cv.warpPerspective(img1, M, (img2.shape[1],img2.shape[0]))\n",
        "cv.imwrite(\"task1 pano.jpg\",imgQ42)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}